{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size =  99\n"
     ]
    }
   ],
   "source": [
    "# For quick indexing, Get vocabulary, and generate idx2ch and ch2idx\n",
    "# Generate Xs and Ys\n",
    "\n",
    "filename = \"data/paulg/paulg.txt\"\n",
    "ckpt_path = 'ckpt/'\n",
    "seq_len = 20\n",
    "\n",
    "f = open(file=filename)\n",
    "lines = f.readlines()\n",
    "\n",
    "raw_data = '\\n'.join(lines) # Full raw data\n",
    "\n",
    "vocab = sorted(list(set(raw_data))) \n",
    "print ('Vocabulary Size = ', len(vocab))\n",
    "\n",
    "idx2ch = vocab\n",
    "ch2idx = {ch:i for i,ch in enumerate(idx2ch)}\n",
    "\n",
    "num_input_sequences = len(raw_data) // seq_len\n",
    "X = np.zeros(shape=[num_input_sequences, seq_len], dtype=np.int32)\n",
    "Y = np.zeros(shape=[num_input_sequences, seq_len], dtype=np.int32)\n",
    "for i in range(num_input_sequences):\n",
    "    X[i] = np.array(  [  ch2idx[ch] for ch in raw_data[seq_len*i : seq_len*(i+1)]  ]  )\n",
    "    Y[i] = np.array(  [  ch2idx[ch] for ch in raw_data[seq_len*i+1 : seq_len*(i+1)+1]  ]  )\n",
    "    \n",
    "X = X.astype(np.int32)\n",
    "Y = Y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params\n",
    "hidden_state_size = 256\n",
    "num_classes = len(idx2ch)\n",
    "state_size = hidden_state_size\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Generation\n",
    "# Use a Generator for batch generation\n",
    "\n",
    "# @Vikash Learn the usage of arange, Learn usage of sample, learn usage of generators\n",
    "def batch_generator():\n",
    "    while True:\n",
    "        sample_idx = random.sample(list(np.arange(len(X))), BATCH_SIZE)\n",
    "        yield X[sample_idx], Y[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define step function to be used in tf.scan\n",
    "def step(hprev, xt):\n",
    "    #initializer\n",
    "    xav_init = tf.contrib.layers.xavier_initializer()\n",
    "    # params\n",
    "    W = tf.get_variable('W', shape=[state_size, state_size], initializer=xav_init)\n",
    "    U = tf.get_variable('U', shape=[state_size, state_size], initializer=xav_init)\n",
    "    b = tf.get_variable('b', shape=[state_size], initializer=tf.constant_initializer(value=0.))\n",
    "    # Current hidden state\n",
    "    h = tf.tanh(tf.matmul(hprev, W) + tf.matmul(xt, U) + b)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow variables declaration\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # inputs\n",
    "    xs_ = tf.placeholder(shape=[None, None], dtype=np.int32) # BATCH_SIZE * seq_len (will contain indices of characters)\n",
    "    ys_ = tf.placeholder(shape=[None], dtype=np.int32) \n",
    "\n",
    "    # embeddings\n",
    "    embeddings = tf.get_variable('emb', shape=[num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, xs_) # rnn_inputs.shape = BATCH_SIZE * seq_len * state_size\n",
    "\n",
    "    # initial_hidden_state\n",
    "    initial_state = tf.placeholder(shape=[None, state_size], dtype=np.float32, name='initial_state') # BATCH_SIZE * state_size\n",
    "\n",
    "    # Scan operation\n",
    "    states = tf.scan(fn=step, \n",
    "                     elems=tf.transpose(rnn_inputs, [1, 0, 2]),\n",
    "                     initializer=initial_state)\n",
    "\n",
    "    # set last state\n",
    "    last_state = states[-1]\n",
    "    states = tf.transpose(states, [1, 0, 2])\n",
    "\n",
    "    # Output weights\n",
    "    V = tf.get_variable('V', shape=[state_size, num_classes],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # Output bias\n",
    "    bo = tf.get_variable('bo', shape=[num_classes],\n",
    "                         initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # Flatten states to 2-d matrix to be multiplied with V\n",
    "    states_flattened = tf.reshape(tensor=states, shape=[-1, state_size])\n",
    "    logits = tf.matmul(states_flattened, V) + bo\n",
    "    predictions = tf.nn.softmax(logits=logits)\n",
    "\n",
    "    # Calculate loss, Optimization step\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=ys_)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss : 0.004632154941558838\n",
      "[0] loss : 0.004332333564758301\n",
      "[0] loss : 0.007811197757720947\n",
      "[0] loss : 0.01008820629119873\n",
      "[0] loss : 0.010538671493530273\n",
      "[0] loss : 0.013271852493286133\n",
      "[0] loss : 0.012941525459289551\n",
      "[0] loss : 0.013567525863647461\n",
      "[0] loss : 0.013936691284179688\n",
      "[0] loss : 0.01430539894104004\n",
      "[0] loss : 0.017343135833740236\n",
      "[0] loss : 0.015680761337280275\n",
      "[0] loss : 0.012917587280273438\n",
      "[0] loss : 0.01058963394165039\n",
      "[0] loss : 0.011009364128112794\n",
      "[0] loss : 0.011745492935180664\n",
      "[0] loss : 0.011423173904418946\n",
      "[0] loss : 0.012323342323303222\n",
      "[0] loss : 0.012547677040100097\n",
      "[0] loss : 0.01267109203338623\n",
      "[0] loss : 0.012278104782104492\n",
      "[0] loss : 0.010780665397644043\n",
      "[0] loss : 0.010450400352478028\n",
      "[0] loss : 0.010386734008789063\n",
      "[0] loss : 0.009539385795593262\n",
      "[0] loss : 0.009310853004455567\n",
      "[0] loss : 0.009513322830200194\n",
      "[0] loss : 0.008723392486572265\n",
      "[0] loss : 0.008196059226989747\n",
      "[0] loss : 0.007955795288085937\n",
      "[0] loss : 0.007575365543365478\n",
      "[0] loss : 0.00751014232635498\n",
      "[0] loss : 0.007200509548187256\n",
      "[0] loss : 0.006927131652832031\n",
      "[0] loss : 0.006669573783874512\n",
      "[0] loss : 0.006751687526702881\n",
      "[0] loss : 0.0069992094039917\n",
      "[0] loss : 0.006366273880004883\n",
      "[0] loss : 0.006450634956359863\n",
      "[0] loss : 0.005911822319030762\n",
      "[0] loss : 0.006531539916992187\n",
      "[0] loss : 0.006411285400390625\n",
      "[0] loss : 0.005871917247772217\n",
      "[0] loss : 0.0058942861557006836\n",
      "[0] loss : 0.005911924839019775\n",
      "[0] loss : 0.006174640655517578\n",
      "[0] loss : 0.006170384407043457\n",
      "[0] loss : 0.005881449699401855\n",
      "[0] loss : 0.006047511577606201\n",
      "[0] loss : 0.006010995388031006\n",
      "[0] loss : 0.006619115352630615\n",
      "[0] loss : 0.005721224308013916\n",
      "[0] loss : 0.005766489028930664\n",
      "[0] loss : 0.005146429061889648\n",
      "[0] loss : 0.005354029655456543\n",
      "[0] loss : 0.005564010620117187\n",
      "[0] loss : 0.00514318323135376\n",
      "[0] loss : 0.005616782188415527\n",
      "[0] loss : 0.005458577156066895\n",
      "[0] loss : 0.005155496120452881\n",
      "[0] loss : 0.004498360633850097\n",
      "[0] loss : 0.004687602043151855\n",
      "[0] loss : 0.004560988426208496\n",
      "[0] loss : 0.004600090980529785\n",
      "[0] loss : 0.004337878227233887\n",
      "[0] loss : 0.00406112003326416\n",
      "[0] loss : 0.004316131591796875\n",
      "[0] loss : 0.003945823669433594\n",
      "[0] loss : 0.003885373592376709\n",
      "[0] loss : 0.0038728327751159667\n",
      "[0] loss : 0.003869971752166748\n",
      "[0] loss : 0.003863929748535156\n",
      "[0] loss : 0.003814888000488281\n",
      "[0] loss : 0.0037156739234924315\n",
      "[0] loss : 0.0036567161083221437\n",
      "[0] loss : 0.003792389392852783\n",
      "[0] loss : 0.0038390891551971437\n",
      "[0] loss : 0.004051219463348388\n",
      "[0] loss : 0.0036789937019348143\n",
      "[0] loss : 0.0034753570556640624\n",
      "[0] loss : 0.0035831019878387453\n",
      "[0] loss : 0.003538000583648682\n",
      "[0] loss : 0.0033950858116149904\n",
      "[0] loss : 0.003433544874191284\n",
      "[0] loss : 0.0032817187309265136\n",
      "[0] loss : 0.0033573017120361328\n",
      "[0] loss : 0.003405174970626831\n",
      "[0] loss : 0.0033581490516662598\n",
      "[0] loss : 0.0033544628620147707\n",
      "[0] loss : 0.003456494092941284\n",
      "[0] loss : 0.0032076163291931154\n",
      "[0] loss : 0.0034548118114471438\n",
      "[0] loss : 0.0034877533912658692\n",
      "[0] loss : 0.0030837578773498537\n",
      "[0] loss : 0.003266042709350586\n",
      "[0] loss : 0.0033976731300354003\n",
      "[0] loss : 0.0032378089427947997\n",
      "[0] loss : 0.003056988000869751\n",
      "[0] loss : 0.003263198375701904\n",
      "[0] loss : 0.0032136569023132324\n",
      "[0] loss : 0.0034300284385681153\n",
      "[0] loss : 0.00311858606338501\n",
      "[0] loss : 0.003253194332122803\n",
      "[0] loss : 0.0032058188915252686\n",
      "[0] loss : 0.003366264343261719\n",
      "[0] loss : 0.0036772522926330565\n",
      "[0] loss : 0.0032703800201416017\n",
      "[0] loss : 0.0033554034233093263\n",
      "[0] loss : 0.0034494686126708986\n",
      "[0] loss : 0.003241122007369995\n",
      "[0] loss : 0.0031248526573181153\n",
      "[0] loss : 0.0032502143383026125\n",
      "[0] loss : 0.0030646884441375734\n",
      "[0] loss : 0.002976438045501709\n",
      "[0] loss : 0.0030861172676086425\n",
      "[0] loss : 0.003105655908584595\n",
      "[0] loss : 0.003024996280670166\n",
      "[0] loss : 0.0029025750160217283\n",
      "[0] loss : 0.00322395396232605\n",
      "[0] loss : 0.0029428086280822753\n",
      "[0] loss : 0.0028347258567810057\n",
      "[0] loss : 0.002854918718338013\n",
      "[0] loss : 0.0028944840431213378\n",
      "[0] loss : 0.0027915685176849365\n",
      "[0] loss : 0.002730647563934326\n",
      "[0] loss : 0.0027787508964538573\n",
      "[0] loss : 0.0027809760570526123\n",
      "[0] loss : 0.00290867280960083\n",
      "[0] loss : 0.0029365100860595703\n",
      "[0] loss : 0.0031371140480041504\n",
      "[0] loss : 0.0031668534278869627\n",
      "[0] loss : 0.0031272327899932863\n",
      "[0] loss : 0.003162637710571289\n",
      "[0] loss : 0.0031682937145233155\n",
      "[0] loss : 0.003284962177276611\n",
      "[0] loss : 0.0034760823249816893\n",
      "[0] loss : 0.0033723938465118407\n",
      "[0] loss : 0.0037758631706237794\n",
      "[0] loss : 0.00411359167098999\n",
      "[0] loss : 0.0039885611534118655\n",
      "[0] loss : 0.004393570423126221\n",
      "[0] loss : 0.004229586601257324\n",
      "[0] loss : 0.0037406353950500486\n",
      "[0] loss : 0.0037553062438964845\n",
      "[0] loss : 0.004317738533020019\n",
      "[0] loss : 0.004329184532165528\n",
      "[0] loss : 0.003948155164718628\n",
      "[0] loss : 0.004185638427734375\n",
      "[0] loss : 0.0047488660812377926\n",
      "[0] loss : 0.004557627677917481\n",
      "[0] loss : 0.004637396812438964\n",
      "[0] loss : 0.004594653606414795\n",
      "[0] loss : 0.00425132417678833\n",
      "[0] loss : 0.0043290328979492185\n",
      "[0] loss : 0.005102688789367676\n",
      "[0] loss : 0.004517306327819824\n",
      "[0] loss : 0.005030224800109864\n",
      "[0] loss : 0.004847076416015625\n",
      "[0] loss : 0.005309083461761475\n",
      "[0] loss : 0.0048705534934997555\n",
      "[0] loss : 0.005439661502838135\n",
      "[0] loss : 0.00592049503326416\n",
      "[0] loss : 0.006916551113128662\n",
      "[0] loss : 0.007948047637939453\n",
      "[0] loss : 0.007881767272949218\n",
      "[0] loss : 0.008527921676635743\n",
      "[0] loss : 0.0082471284866333\n",
      "[0] loss : 0.007857191562652587\n",
      "[0] loss : 0.006984339714050293\n",
      "[0] loss : 0.0069451088905334475\n",
      "[0] loss : 0.005792019367218018\n",
      "[0] loss : 0.006650750160217285\n",
      "[0] loss : 0.006303930759429931\n",
      "[0] loss : 0.005711911678314209\n",
      "[0] loss : 0.005619765281677246\n",
      "[0] loss : 0.0056572771072387696\n",
      "[0] loss : 0.006115463733673096\n",
      "[0] loss : 0.00661565637588501\n",
      "[0] loss : 0.005414550304412842\n",
      "[0] loss : 0.005962135791778564\n",
      "[0] loss : 0.0062063422203063965\n",
      "[0] loss : 0.0056607270240783695\n",
      "[0] loss : 0.005784321784973145\n",
      "[0] loss : 0.005813920974731445\n",
      "[0] loss : 0.005934146881103516\n",
      "[0] loss : 0.005648284912109375\n",
      "[0] loss : 0.005355167388916016\n",
      "[0] loss : 0.004811491966247559\n",
      "[0] loss : 0.005350676536560059\n",
      "[0] loss : 0.0056724510192871095\n",
      "[0] loss : 0.005115869522094726\n",
      "[0] loss : 0.005502219200134277\n",
      "[0] loss : 0.005502461910247803\n",
      "[0] loss : 0.0054791226387023925\n",
      "[0] loss : 0.0058403000831604\n",
      "[0] loss : 0.006195602416992188\n",
      "[0] loss : 0.006531903266906738\n",
      "[0] loss : 0.005567418575286865\n",
      "[0] loss : 0.005690824508666992\n",
      "[0] loss : 0.00540673828125\n",
      "[0] loss : 0.005399850368499756\n",
      "[0] loss : 0.005185352325439453\n",
      "[0] loss : 0.005100089550018311\n",
      "[0] loss : 0.00481385612487793\n",
      "[0] loss : 0.004748647212982178\n",
      "[0] loss : 0.004839280605316162\n",
      "[0] loss : 0.0049401488304138185\n",
      "[0] loss : 0.005018074989318847\n",
      "[0] loss : 0.005404178619384766\n",
      "[0] loss : 0.0056276068687438965\n"
     ]
    }
   ],
   "source": [
    "# Training time\n",
    "\n",
    "epochs = 50\n",
    "training_set = batch_generator()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # init session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_loss = 0\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            for j in range(1000):\n",
    "                xs, ys = training_set.__next__()\n",
    "                _, train_loss_ = sess.run([train_op, loss], feed_dict = {\n",
    "                        xs_ : xs,\n",
    "                        ys_ : ys.reshape([BATCH_SIZE * seq_len]),\n",
    "                        initial_state : np.zeros([BATCH_SIZE, state_size])\n",
    "                    })\n",
    "                train_loss +=  train_loss_\n",
    "                print('[{}] loss : {}'.format(i,train_loss/1000))\n",
    "                train_loss = 0\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted by user at ' + str(i))\n",
    "        #\n",
    "        # training ends here; \n",
    "        #  save checkpoint\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, ckpt_path + 'vanilla1.ckpt', global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "{\n",
      "______Generated Text_______\n",
      "{t,ytippp. t n. t,yyveyven. t .,yt t t t,yt n. t n. t t t t,yyt,yvyt n. n. t,yn. tippppp. n. n. t, yvy4, tip. t \n"
     ]
    }
   ],
   "source": [
    "random_init_word = random.choice(idx2ch)\n",
    "current_word = ch2idx[random_init_word]\n",
    "print ('Provided random character = ', random_init_word)\n",
    "#   \n",
    "# start session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # init session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # restore session\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
    "    saver = tf.train.Saver()\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #   \n",
    "    # generate operation\n",
    "    words = [current_word]\n",
    "    state = None\n",
    "    # set batch_size to 1\n",
    "    batch_size = 1 \n",
    "    num_words =  111\n",
    "    # enter the loop\n",
    "    for i in range(num_words):\n",
    "        if state:\n",
    "            feed_dict = { xs_ : np.array(current_word).reshape([1, 1]), \n",
    "                    initial_state : state_ }\n",
    "        else:\n",
    "            feed_dict = { xs_ : np.array(current_word).reshape([1,1])\n",
    "                    , initial_state : np.zeros([batch_size, state_size]) }\n",
    "   \n",
    "        # forward propagation\n",
    "        preds, state_ = sess.run([predictions, last_state], feed_dict=feed_dict)\n",
    "        #   \n",
    "        # set flag to true\n",
    "        state = True\n",
    "        # \n",
    "        # set new word\n",
    "        current_word = np.random.choice(preds.shape[-1], 1, p=np.squeeze(preds))[0]\n",
    "        # add to list of words\n",
    "        words.append(current_word)\n",
    "#########\n",
    "# text generation complete\n",
    "#\n",
    "print('______Generated Text_______')\n",
    "print(''.join([idx2ch[w] for w in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
